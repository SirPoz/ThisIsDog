{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00e2b295-4de9-4b30-a9a2-fbc8698a38b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:57:00.932974600Z",
     "start_time": "2023-11-04T14:57:00.722552700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e2df326-5fd6-4491-a6d4-212f720f2077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:57:01.011208900Z",
     "start_time": "2023-11-04T14:57:00.742831400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 944\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'Folder': 'Border Collie', 'Count': 112},\n {'Folder': 'Borzoi', 'Count': 110},\n {'Folder': 'Cocker', 'Count': 130},\n {'Folder': 'German Sheperd', 'Count': 109},\n {'Folder': 'Golden Retriever', 'Count': 127},\n {'Folder': 'Greyhound', 'Count': 109},\n {'Folder': 'Pomeranian', 'Count': 149},\n {'Folder': 'Shiba Inu', 'Count': 98}]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"Train\"\n",
    "test_data_dir = \"Test\"\n",
    "\n",
    "# Define data augmentation and normalization transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomAffine(degrees=0, shear=30),  # Add shear transformation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Add color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the train and test data with transformations\n",
    "dataset = ImageFolder(data_dir, transform=train_transform)\n",
    "test_dataset = ImageFolder(test_data_dir, transform=test_transform)\n",
    "\n",
    "# Print some information about the loaded data\n",
    "train_dogs = []\n",
    "pic_count = 0\n",
    "for folder in dataset.classes:\n",
    "    files = os.listdir(os.path.join(data_dir, folder))\n",
    "    train_dogs.append({\"Folder\": folder, \"Count\": len(files)})\n",
    "    pic_count += len(files)\n",
    "\n",
    "print(\"Number of train data:\", pic_count)\n",
    "train_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a511f3cd-e52c-49db-b09e-434b451895d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:57:01.012216Z",
     "start_time": "2023-11-04T14:57:00.800979800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Data : 844\n",
      "Length of Validation Data : 100\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 32\n",
    "val_size = 100\n",
    "train_size = len(dataset) - val_size \n",
    "\n",
    "train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7100780a-9fb4-4bee-a06e-24c33cdd5307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:57:01.013207700Z",
     "start_time": "2023-11-04T14:57:00.819004300Z"
    }
   },
   "outputs": [],
   "source": [
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(val_data, batch_size*2, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23a3bf1a-f4e9-45c4-90e7-3bfc152160e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:57:01.013207700Z",
     "start_time": "2023-11-04T14:57:00.827002300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc1681c4-47b3-4abb-b251-c0441df8898a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:57:01.013207700Z",
     "start_time": "2023-11-04T14:57:00.853006200Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs] # collect losses for batches\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]    # collect accuracies for batches\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecd22b91-12e0-4b80-b7c4-b1948d3c85db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:57:01.021568400Z",
     "start_time": "2023-11-04T14:57:00.876930800Z"
    }
   },
   "outputs": [],
   "source": [
    "class DogClassification(ImageClassificationBase):\n",
    "    def __init__(self, num_classes = 8):\n",
    "        super(DogClassification, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc = nn.Linear(200704,1024)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512,num_classes)\n",
    "        # TODO softmax as activation for output?\n",
    "        #  âˆ˜ LeNet-5\n",
    "    \n",
    "        # resources to read up on architectures:\n",
    "        # https://medium.com/@siddheshb008/lenet-5-architecture-explained-3b559cb2d52b\n",
    "        # https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/\n",
    "        \n",
    "        # more general resources\n",
    "        # https://medium.com/@siddheshb008/why-convolutions-dd7641d2bf81\n",
    "        # https://medium.com/@siddheshb008/understanding-convolution-neural-networks-a30211e12a06\n",
    "        # https://medium.com/@siddheshb008/understanding-convolutional-neural-networks-part-2-98694dd47923\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        out = self.layer1(xb)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d55718a-8d64-485e-87c6-3d55d27b680f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T14:57:01.023566200Z",
     "start_time": "2023-11-04T14:57:00.908594400Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f62ff860-39e8-4a6d-9a1b-e5d8509de678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:25:04.359179900Z",
     "start_time": "2023-11-04T14:57:00.925958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.5884, val_loss: 2.0841, val_acc: 0.2543\n",
      "Epoch [1], train_loss: 1.9327, val_loss: 1.8277, val_acc: 0.3385\n",
      "Epoch [2], train_loss: 1.8246, val_loss: 1.7780, val_acc: 0.3368\n",
      "Epoch [3], train_loss: 1.6459, val_loss: 1.6056, val_acc: 0.3767\n",
      "Epoch [4], train_loss: 1.5402, val_loss: 1.7009, val_acc: 0.3724\n",
      "Epoch [5], train_loss: 1.5926, val_loss: 1.5901, val_acc: 0.4253\n",
      "Epoch [6], train_loss: 1.6210, val_loss: 1.7061, val_acc: 0.3724\n",
      "Epoch [7], train_loss: 1.5381, val_loss: 1.5526, val_acc: 0.4470\n",
      "Epoch [8], train_loss: 1.4371, val_loss: 1.6476, val_acc: 0.3524\n",
      "Epoch [9], train_loss: 1.4785, val_loss: 1.5294, val_acc: 0.4288\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.0001\n",
    "model = DogClassification() # number of breeds\n",
    "\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3eabc2ed-e016-418e-9813-0c1c81257fba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:25:04.453179800Z",
     "start_time": "2023-11-04T15:25:04.436299Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_func2 = torch.optim.AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12120a4b-7346-48eb-8b0c-ff34120de4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:59:34.727139300Z",
     "start_time": "2023-11-04T15:25:04.461183600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 1.5672, val_loss: 1.6020, val_acc: 0.3759\n",
      "Epoch [1], train_loss: 1.4079, val_loss: 1.6707, val_acc: 0.4132\n",
      "Epoch [2], train_loss: 1.4882, val_loss: 1.6366, val_acc: 0.3698\n",
      "Epoch [3], train_loss: 1.4379, val_loss: 1.5122, val_acc: 0.4175\n",
      "Epoch [4], train_loss: 1.3950, val_loss: 1.5292, val_acc: 0.4748\n",
      "Epoch [5], train_loss: 1.3084, val_loss: 1.5997, val_acc: 0.4115\n",
      "Epoch [6], train_loss: 1.3100, val_loss: 1.6297, val_acc: 0.4349\n",
      "Epoch [7], train_loss: 1.3070, val_loss: 1.4867, val_acc: 0.4132\n",
      "Epoch [8], train_loss: 1.3234, val_loss: 1.4418, val_acc: 0.5139\n",
      "Epoch [9], train_loss: 1.2771, val_loss: 1.4277, val_acc: 0.4861\n"
     ]
    }
   ],
   "source": [
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24b53d8e-ea7c-4d86-aa6a-e82a689eb7cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:59:34.744270400Z",
     "start_time": "2023-11-04T15:59:34.730137700Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
