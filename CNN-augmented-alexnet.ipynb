{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e2b295-4de9-4b30-a9a2-fbc8698a38b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:21:08.140280500Z",
     "start_time": "2023-11-04T15:20:59.828085800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2df326-5fd6-4491-a6d4-212f720f2077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:21:10.379081500Z",
     "start_time": "2023-11-04T15:21:10.326140200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 944\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'Folder': 'Border Collie', 'Count': 112},\n {'Folder': 'Borzoi', 'Count': 110},\n {'Folder': 'Cocker', 'Count': 130},\n {'Folder': 'German Sheperd', 'Count': 109},\n {'Folder': 'Golden Retriever', 'Count': 127},\n {'Folder': 'Greyhound', 'Count': 109},\n {'Folder': 'Pomeranian', 'Count': 149},\n {'Folder': 'Shiba Inu', 'Count': 98}]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"Train\"\n",
    "test_data_dir = \"Test\"\n",
    "\n",
    "# Define data augmentation and normalization transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomAffine(degrees=0, shear=30),  # Add shear transformation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Add color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the train and test data with transformations\n",
    "dataset = ImageFolder(data_dir, transform=train_transform)\n",
    "test_dataset = ImageFolder(test_data_dir, transform=test_transform)\n",
    "\n",
    "# Print some information about the loaded data\n",
    "train_dogs = []\n",
    "pic_count = 0\n",
    "for folder in dataset.classes:\n",
    "    files = os.listdir(os.path.join(data_dir, folder))\n",
    "    train_dogs.append({\"Folder\": folder, \"Count\": len(files)})\n",
    "    pic_count += len(files)\n",
    "\n",
    "print(\"Number of train data:\", pic_count)\n",
    "train_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a511f3cd-e52c-49db-b09e-434b451895d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:21:12.030780600Z",
     "start_time": "2023-11-04T15:21:11.636355200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Data : 844\n",
      "Length of Validation Data : 100\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 32\n",
    "val_size = 100\n",
    "train_size = len(dataset) - val_size \n",
    "\n",
    "train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7100780a-9fb4-4bee-a06e-24c33cdd5307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:21:13.310397200Z",
     "start_time": "2023-11-04T15:21:13.129855300Z"
    }
   },
   "outputs": [],
   "source": [
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(val_data, batch_size*2, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a3bf1a-f4e9-45c4-90e7-3bfc152160e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:21:14.209166200Z",
     "start_time": "2023-11-04T15:21:14.153815500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1681c4-47b3-4abb-b251-c0441df8898a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:21:15.776144900Z",
     "start_time": "2023-11-04T15:21:15.737506500Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs] # collect losses for batches\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]    # collect accuracies for batches\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd22b91-12e0-4b80-b7c4-b1948d3c85db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:22:29.974101100Z",
     "start_time": "2023-11-04T15:22:29.842107600Z"
    }
   },
   "outputs": [],
   "source": [
    "class DogClassification(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(200704,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256,8),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "        #  âˆ˜ AlexNet\n",
    "        # resources to read up on architectures:\n",
    "        # https://medium.com/analytics-vidhya/concept-of-alexnet-convolutional-neural-network-6e73b4f9ee30\n",
    "        # modelled after this:\n",
    "        # https://github.com/abhijeetpujara/AlexNet/blob/main/Alexnet.ipynb\n",
    "        \n",
    "        # more general resources\n",
    "        # https://medium.com/@siddheshb008/why-convolutions-dd7641d2bf81\n",
    "        # https://medium.com/@siddheshb008/understanding-convolution-neural-networks-a30211e12a06\n",
    "        # https://medium.com/@siddheshb008/understanding-convolutional-neural-networks-part-2-98694dd47923\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d55718a-8d64-485e-87c6-3d55d27b680f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T15:22:32.417092400Z",
     "start_time": "2023-11-04T15:22:30.614631900Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f62ff860-39e8-4a6d-9a1b-e5d8509de678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T16:08:21.130662600Z",
     "start_time": "2023-11-04T15:22:33.925375100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.0198, val_loss: 2.0696, val_acc: 0.1675\n",
      "Epoch [1], train_loss: 1.9429, val_loss: 1.8922, val_acc: 0.4392\n",
      "Epoch [2], train_loss: 1.8858, val_loss: 1.8253, val_acc: 0.4766\n",
      "Epoch [3], train_loss: 1.8627, val_loss: 1.8938, val_acc: 0.3542\n",
      "Epoch [4], train_loss: 1.8331, val_loss: 1.8270, val_acc: 0.4844\n",
      "Epoch [5], train_loss: 1.8261, val_loss: 1.7971, val_acc: 0.5078\n",
      "Epoch [6], train_loss: 1.8134, val_loss: 1.7465, val_acc: 0.5356\n",
      "Epoch [7], train_loss: 1.7838, val_loss: 1.8139, val_acc: 0.5200\n",
      "Epoch [8], train_loss: 1.7878, val_loss: 1.8241, val_acc: 0.4670\n",
      "Epoch [9], train_loss: 1.7952, val_loss: 1.8232, val_acc: 0.4505\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.0001\n",
    "model = DogClassification() # number of breeds\n",
    "\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eabc2ed-e016-418e-9813-0c1c81257fba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T16:12:41.436415500Z",
     "start_time": "2023-11-04T16:12:41.430413600Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_func2 = torch.optim.AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12120a4b-7346-48eb-8b0c-ff34120de4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-04T16:41:19.544500700Z",
     "start_time": "2023-11-04T16:12:45.657624900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 1.7987, val_loss: 1.7709, val_acc: 0.5356\n",
      "Epoch [1], train_loss: 1.7707, val_loss: 1.8036, val_acc: 0.4800\n",
      "Epoch [2], train_loss: 1.7514, val_loss: 1.7913, val_acc: 0.5200\n",
      "Epoch [3], train_loss: 1.7450, val_loss: 1.7668, val_acc: 0.5512\n",
      "Epoch [4], train_loss: 1.7530, val_loss: 1.7571, val_acc: 0.5530\n",
      "Epoch [5], train_loss: 1.7525, val_loss: 1.8029, val_acc: 0.4705\n",
      "Epoch [6], train_loss: 1.7251, val_loss: 1.7863, val_acc: 0.4488\n",
      "Epoch [7], train_loss: 1.6971, val_loss: 1.7423, val_acc: 0.5477\n",
      "Epoch [8], train_loss: 1.7136, val_loss: 1.7798, val_acc: 0.4922\n",
      "Epoch [9], train_loss: 1.7068, val_loss: 1.7446, val_acc: 0.5330\n"
     ]
    }
   ],
   "source": [
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b53d8e-ea7c-4d86-aa6a-e82a689eb7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
